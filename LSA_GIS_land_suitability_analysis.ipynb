{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTLDs5LO-nQu"
   },
   "source": [
    "# **Land Suitability Analysis for Post Disaster Housing Relocation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EL7vI4D8-wBp"
   },
   "source": [
    "## **Introduction**\n",
    "This project is building upon the process developed by the [Hurricane Matthew Disaster Recovery Initiative](https://coastalresiliencecenter.unc.edu \"coastal resilience\") detailed in the [Technical Memo (1)](https://coastalresiliencecenter.unc.edu/wp-content/uploads/sites/845/2018/12/LSA-Technical-Memo-6.pdf). This Jupyter Notebook lays out an open-source methodology for a Land Suitability Analysis for Post-disaster Housing Relocation using QGIS and PyQGIS. This project was completed under the 2021 Google Summer of Code. \n",
    "\n",
    "### What is a Land Suitability Analysis?\n",
    "A Land Suitability Analysis is a land-use planning tool that uses geographic information systems (GIS) to identify potential areas for redevelopment, using a set of variables with specified criteria and weights, which identify parcels with a reduced risk to flooding, are within the municipal limts, and help meet other community development goals [(1)](https://coastalresiliencecenter.unc.edu/wp-content/uploads/sites/845/2018/12/LSA-Technical-Memo-6.pdf). A land suitability analysis can use either vector (points, lines, and polygons) or raster (imagery, slope) data. This land suitability analysis will only use vectorized data. If the dataset you have chosen is not a vector layer, convert it to one using the [Vector to Raster QGIS tool](https://docs.qgis.org/2.14/en/docs/training_manual/complete_analysis/raster_to_vector.html). This project also assumes that geopackages are the default file type. Convert your datasets to a geopackage if necessary. \n",
    "\n",
    "\n",
    "A Land Suitability Analysis is only one tool that planners and emergency managers have to mitigate disasters. This Land Suitability Analysis is part of a multi-step process that involves community leaders and organizers. This analysis is being developed in collaboration with FEMA supported Regional Catastrophic Emergency Management planners and MIT's Urban Risk Lab. \n",
    "\n",
    "### Regional Catastrophic Preparedness Grant Initiative\n",
    "\n",
    "The Regional Catastrophic Preparadness Grant Initiative plays an important role in the National Preparedness Initiative. RCPGP supports the building of core capabilities essential to the goal of a secure and resilient nation by providing resources to close known cability gaps.\n",
    "\n",
    "The City of Boston, Massachusetts Emergency Management Agency, and the Metro-Boston Homeland Security Region have received the Federal Emergency Management Agency’s Regional Catastrophic Preparedness Grant with the focus being disaster housing.\n",
    "\n",
    "As the coordinating agency for the Regional Catastrophic Preparedness Grant Program (RCPGP), Boston’s Office of Emergency Management will create MBHSR Disaster Housing Working Groups to guide the development of a plan identifying disaster housing gaps.\n",
    "\n",
    "Boston’s Office of Emergency Management has partnered with MIT’s Urban Risk Lab to serve as a pilot community for their Housing Pre-Planning Toolkit research project in the first phase of the grant program. The goal is to create an interactive tool for the City and MBHSR that presents information related to housing resilience and recovery that will assist communities in identifying and organizing information related to their unique housing environment and priorities for housing resilience and recovery.\n",
    "\n",
    "Through this effort, we will set common objectives in housing policy, identify local hazards and vulnerabilities, and introduce federal disaster and hazard mitigation assistance programs to assist localities in creating plans for disaster-related housing and mitigation.\n",
    "\n",
    "As part of the MBHSR Disaster Housing Working Groups, we will host Housing Pre-Planning Toolkit workshops in order to gather and coordinate the information needed among the MBSHR jurisdictions. We have put together a list of stakeholders and subject matter experts that will be critical throughout the planning process to participate in the MBHSR Disaster Housing Working Groups and workshops.\n",
    "\n",
    "### About the City of Boston\n",
    "\n",
    "3 of the top 5 U.S. cities with the most affordable housing vulnerable to coastal flooding are in Massachusetts: Revere, Boston, and Quincy [(2)](https://iopscience.iop.org/article/10.1088/1748-9326/abb266). About 17% of Boston is built on landfill that was once tidal flats, marshes, or water which is vulnerable to flooding today [(3)](https://www.biblio.com/book/gaining-ground-history-landmarking-boston-seashole/d/1199219300). Boston's sea level may rise 40 inches by the 2070s [(4)](https://www.boston.gov/sites/default/files/embed/file/2018-09/climatereadysouthboston_execsum_v9.1s_web.pdf). The datasets we chose to include in this initial analysis reflect this reality. \n",
    "\n",
    "For more information, read [The 1-2-3s of Boston's Rising Sea Level](https://www.wbur.org/news/2021/06/15/boston-climate-change-sea-level-rise-numbers), [8 Maps that Explain Boston's Changing Shoreline](https://www.wbur.org/news/2021/06/14/8-maps-that-explain-bostons-changing-shoreline), or [Resilient Boston Harbor](https://www.boston.gov/environment-and-energy/resilient-boston-harbor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQ_cxAyfIy0L"
   },
   "source": [
    "### **Data**\n",
    "\n",
    "\n",
    "All datasets used in this example are publicly available datasets. To find Boston specific data visit our [Open Data Portal](https://data.boston.gov). Below are the datasets that were considered for use in the original Coastal Resilience Center analysis [(1)](https://coastalresiliencecenter.unc.edu/wp-content/uploads/sites/845/2018/12/LSA-Technical-Memo-6.pdf). Each community is different and the datasets that you decide to use may differ. We encourage everyone to talk through the problems that would face your community in a disaster housing sitaution and choose datasets that compliment them. \n",
    "\n",
    "\n",
    "\n",
    "| Category | Criteria | Source | Used in LSA |\n",
    "| --- | --- | --- | --- |\n",
    "| __Accesibility of Services and Facilities__ | Existing Jurisdiction Proximity | Census | * |\n",
    "| | Proximity to commercial area | Local/Plans | |\n",
    "| | School proximity | Census | |\n",
    "| |Hospital proximity | Census | |\n",
    "| | Utility Infrastructure Connectivity | County/State | |\n",
    "| | Park/Playground Proximity | Local | |\n",
    "|__Transportation__| Bus Stop Proximity | Local | |\n",
    "| | Major Highway Proximity | Census | |\n",
    "| __Socioeconomic Factors__ | Population Density | Census | |\n",
    "| | Community Preference | Survey | |\n",
    "| | Renter/Owner | Census | |\n",
    "| | Neighborhood Type | Local | |\n",
    "| | AFN Individuals | Local | |\n",
    "| | Land Value | Census | |\n",
    "| __Environment and Safety__ | Protective Infrastructure Integrity | Local | |\n",
    "| | Drainage | Survey/Local | |\n",
    "| | Reliance on Protective Infrastructure | Local | |\n",
    "| | Proximity to Water Bodies | State | | \n",
    "| __Topography__ | Slope | USGS |  |\n",
    "| | DEM | USGS | |\n",
    "| | Water Table Depth | USGS | |\n",
    "| | Tidal Factors | USGS | |\n",
    "| | Soil Composition | SSURGO | |\n",
    "| | Vegetaion Composition | State | |\n",
    "| | Vegetation Density | State | |\n",
    "| __Planning__ | Areas of Future Development | Local | * |\n",
    "| | Land/Building Vacancy | Local | * |\n",
    "| | Large Infrastructure Projects | Local | |\n",
    "| | Economic Development Areas | Local | |\n",
    "| __Flood Risk__ | Historical Value/Significance | Local/Survey | |\n",
    "| | FEMA Flood Zones (100 + 500 Yr) | FEMA | * |\n",
    "| | Sea Level Rise | NOAA | * |\n",
    "| | Historical Hurricane/Flooding | NOAA | |\n",
    "\n",
    "\n",
    "\n",
    "### Software\n",
    "The only software required for this project is QGIS and the necessary python 3.7 libraries. The notebook can be run in any operating system (Windows, OS X, or Linux). The only modifications necessary are to system paths which will be clearly identified. We recommend using [OSGEO Live Linux Distribution](https://live.osgeo.org/en/index.html) as it contains the necessary software and libraries pre-installed. This notebook will default to this operating systems location to avoid confusion. We run the notebook using a virtual machine running OSGEO Live on a Windows PC.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize QGIS Resources\n",
    "We first need to initialize the QGIS resources at the beginning of our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "\n",
    "# QGIS core functionality imports\n",
    "from qgis import *\n",
    "from qgis.core import *\n",
    "from qgis.core import QgsApplication\n",
    "from qgis.core import QgsExpression\n",
    "from qgis.core import QgsVectorDataProvider\n",
    "from qgis.core import QgsVectorLayer\n",
    "from qgis.PyQt.QtCore import QVariant\n",
    "from qgis.utils import iface\n",
    "from PyQt5 import QtCore\n",
    "from PyQt5 import QtGui\n",
    "from PyQt5 import QtWidgets\n",
    "from PyQt5.QtCore import QFileInfo\n",
    "# QGIS Image tools\n",
    "from PyQt5.QtGui import QImage\n",
    "from PyQt5.QtCore import QSize\n",
    "from PyQt5.QtGui import QColor\n",
    "from PyQt5.QtGui import QPainter\n",
    "from qgis.core import QgsRectangle\n",
    "# QGIS Processing tools imports\n",
    "sys.path.append('/usr/share/qgis/python/plugins')\n",
    "from processing.core.Processing import Processing\n",
    "Processing.initialize()\n",
    "from processing.tools import *\n",
    "# Data manipulation imports\n",
    "import pandas as pd\n",
    "# Plot images imports\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Locating your layers** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have gathered the datasets you'll use for your analysis, we need to store the path of the folder where these layers are located and a location to store our output data as a variable. This tells the script where to find the layers. The following script will create folders in your computer if they are not already there. Under line 2, insert the location for where you would like these folders created and your layers stored. Copy and paste the layers you've found online into the input_layers_path folder. \n",
    "\n",
    "* **DATA_PATH**: This variable will contain the general path where the folder with the layers is contained and also where you want to store the output files that are going to be generated during the analysis. You should change this accordingly to where your data is located.\n",
    "* **INPUT_LAYERS_PATH**: This variable contains the *DATA_PATH* direction and we are adding the specific name of the folder where your layers are stored. You may change the folder name(input_layers) to the name of your folder if it's different.\n",
    "* **OUTPUT_FILES_PATH**: This variable contains the *DATA_PATH* direction and we are adding the name of the folder where the generated csv files will be stored, you don't need to have this folder created, since it will be created automatically by the script. You can change the default folder name (output_file) if you want.\n",
    "* **OUTPUT_LAYERS_PATH**: This variable contains the *DATA_PATH* direction and we are adding the name of the folder where the generated layers will be stored, you don't need to have this folder created, since it will be created automatically by the script. You can change the default folder name (output_layers) if you want.\n",
    "\n",
    "``As you can see, you can write the whole path directly to the variables. But, we decided to have the variable DATA_PATH in case this path changes, you'll need only to change this variable and not all of them.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these path accordingly to where your data folders are located\n",
    "DATA_PATH = \"/home/daniel/QGIS Projects/LSA/\" \n",
    "INPUT_LAYERS_PATH = DATA_PATH + \"input_layers/\"\n",
    "OUTPUT_FILES_PATH = DATA_PATH + \"output_files/\"\n",
    "# Change this path with the name of the folder where you'll save the output layers (It'll be created automatically)\n",
    "OUTPUT_LAYERS_PATH = DATA_PATH + \"output_layers/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said in the previous section, you don't need to have the output_files and output_layer folders created on your computer. To do that automatically, the function **create_directory_if_do_not_exist** verify if these folders are already created in your machine, if not, it creates them for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will create a folder if it doesn't exist\n",
    "def create_directory_if_do_not_exist(dirName):\n",
    "    if not os.path.exists(dirName):\n",
    "        os.mkdir(dirName)\n",
    "        print(f\"Directory {dirName} created \")\n",
    "    else:    \n",
    "        print(f\"Directory {dirName} \\033[1m already exists \\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directory_if_do_not_exist(OUTPUT_LAYERS_PATH)\n",
    "create_directory_if_do_not_exist(OUTPUT_FILES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Instance\n",
    "We are working in a standalone PyQGIS script, so we are going to instantiate the layers from our QGIS project. In **line 1,** change the location of your home folder. This could be /home/user/ or simply \"~\". In **line 3,** change the location of your temporary folder. On Windows this is /AppData/Local/Temp. On Linux this could be /tmp. In **line 8,** paste the location of your .qgz QGIS project file. In your QGIS project, load the layers that you are going to use for your analysis in to the project and save it first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = expanduser(\"/home/user/\")\n",
    "QgsApplication( [], False, home + \"/tmp\" )\n",
    "QgsApplication.setPrefixPath(\"/usr/lib/qgis\", True)\n",
    "app = QtWidgets.QApplication([])\n",
    "\n",
    "QgsApplication.initQgis()\n",
    "\n",
    "fname = r\"/home/daniel/QGIS Projects/LSA/lsa.qgz\"\n",
    "\n",
    "# Get the project instance\n",
    "project = QgsProject.instance()\n",
    "# Open the project\n",
    "project.read(fname)\n",
    "print(project.fileName())\n",
    "\n",
    "map_layers = QgsProject.instance().mapLayers()\n",
    "print(map_layers) #This now has the layers in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing layers names into a list\n",
    "layer_names = list(map_layers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print layers names\n",
    "for i, layer in enumerate(layer_names):\n",
    "    print(i, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to verify the name of the layers and change them if you have named them in a different manner. If any of your layers paths are not found, you'll see an error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing each layer in an independent variable. If you are using more than 3 layers, add more lines\n",
    "sea_level_rise_layer = map_layers[layer_names[0]]\n",
    "flz_haz_layer = map_layers[layer_names[1]]\n",
    "boston_zoning_layer = map_layers[layer_names[2]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **calculate_layer_area** receives two inputs:\n",
    "1. **layer**: layer to add area calculation field\n",
    "2. **output_field_name**: name of the new column that is going to be created in the layer features table.\n",
    "What the function does is that it'll create a new column with the specified output name. Then, it will calculate the area of each parcel in the layer and add the values in this new created column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an expression for AREA calculation\n",
    "area_expression = QgsExpression(\"$area\")\n",
    "\n",
    "def calculate_layer_area(layer, output_field_name):\n",
    "    # We need to pass the path where the data is located, a layer name and a provider \n",
    "    # In this case org is the correct provider for geopackages.\n",
    "\n",
    "    provider = layer.dataProvider()\n",
    "    provider.addAttributes([QgsField(output_field_name, QVariant.Double)])\n",
    "\n",
    "    # Need to give context where it occurs\n",
    "    context = QgsExpressionContext()\n",
    "    context.appendScopes(QgsExpressionContextUtils.globalProjectLayerScopes(layer))\n",
    "\n",
    "    with edit(layer):\n",
    "        for f in layer.getFeatures():\n",
    "            context.setFeature(f)\n",
    "            f[output_field_name] = area_expression.evaluate(context)\n",
    "            layer.updateFeature(f)\n",
    "      \n",
    "\n",
    "    \n",
    "# BOSTON ZONING layer - area calculation\n",
    "calculate_layer_area(boston_zoning_layer, \"boston_polyg_area_4\")\n",
    "\n",
    "# FLZ_HAZ layer - area calculation\n",
    "calculate_layer_area(flz_haz_layer, \"flz_b_polyg_area_4\")\n",
    "\n",
    "# Sea level rise layer - area calculation\n",
    "calculate_layer_area(sea_level_rise_layer, \"searise_polyg_area_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPq1DkpTN26_"
   },
   "source": [
    "## **Intersect Layers** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To intersect to layers you'll need to run **Processing.runAlgoritm** function, it receives two parameters:\n",
    "1. **Algorithm ID**: QGIS has different vector overlay functionalities, since we want to perform intersections between two layers you'll need to pass \"qgis:intersection\" id. For reference, you can check this link: https://docs.qgis.org/3.16/en/docs/user_manual/processing_algs/qgis/vectoroverlay.html\n",
    "2. **Dictionary**: Dictionary with all the parameters for performing intersection, in this case those are: input layer, overlay layer and output layer name. You can change output layers name or keep the names that are written below, as you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intersect Flood Hazard layer with Boston Zoning layer\n",
    "parameters_boston_flz = {\n",
    "       'INPUT': boston_zoning_layer,\n",
    "       'OVERLAY': flz_haz_layer,\n",
    "       'OUTPUT': OUTPUT_LAYERS_PATH + 'intersection_flz_boston.gpkg'\n",
    "}\n",
    "Processing.runAlgorithm(\"qgis:intersection\", parameters_boston_flz)\n",
    "\n",
    "#Intersect Sea Rise layer with Boston Zoning layer\n",
    "parameters_boston_searise = {\n",
    "       'INPUT': boston_zoning_layer,\n",
    "       'OVERLAY': sea_level_rise_layer,\n",
    "       'OUTPUT': OUTPUT_LAYERS_PATH + 'intersection_searise_boston.gpkg'\n",
    "}\n",
    "Processing.runAlgorithm(\"qgis:intersection\", parameters_boston_searise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export layers as CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After intersecting the layer, we'll need to use them for the analysis. To do so, we are loading the layers into variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersected layers to export\n",
    "intersection_flz_boston_layer = QgsVectorLayer(f\"{OUTPUT_LAYERS_PATH}intersection_flz_boston.gpkg\", \"ogr\")\n",
    "intersection_searise_boston_layer = QgsVectorLayer(f\"{OUTPUT_LAYERS_PATH}intersection_searise_boston.gpkg\", \"ogr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to perform some data manipulation to the layers features tables, it'll be easier to work with python and loading the data into dataframes. That's why the next step will be to save the layers features into csv files.\n",
    "\n",
    "The function **QgsVectorFileWriter.writeAsVectorFormat** will do that work for you, and it needs the following inputs:\n",
    "1. **layer**: layer to write.\n",
    "2. **csv file name**: file name to write to. File name can be changed.\n",
    "3. **encoding**: encoding to use.\n",
    "4. **driverName**: OGR driver to use.\n",
    "5. **layerOptions**: list of OGR layer creation options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export boston zoning layer as csv file\n",
    "QgsVectorFileWriter.writeAsVectorFormat(\n",
    "    boston_zoning_layer, \n",
    "    f\"{OUTPUT_FILES_PATH}boston_zoning.csv\", \n",
    "    \"utf-8\", \n",
    "    driverName = \"CSV\", \n",
    "    layerOptions = ['GEOMETRY=AS_XYZ']\n",
    ")\n",
    "\n",
    "# Export intersect layer between boston zoning layer and flood hazard layer as csv file\n",
    "QgsVectorFileWriter.writeAsVectorFormat(\n",
    "    intersection_flz_boston_layer, \n",
    "    f\"{OUTPUT_FILES_PATH}intersection_flz_boston.csv\", \n",
    "    \"utf-8\", \n",
    "    driverName = \"CSV\", \n",
    "    layerOptions = ['GEOMETRY=AS_XYZ']\n",
    ")\n",
    "\n",
    "# Export intersect layer between boston zoning layer and sea level rise layer as csv file\n",
    "QgsVectorFileWriter.writeAsVectorFormat(\n",
    "    intersection_searise_boston_layer, \n",
    "    f\"{OUTPUT_FILES_PATH}intersection_searise_boston.csv\", \n",
    "    \"utf-8\", \n",
    "    driverName = \"CSV\", \n",
    "    layerOptions = ['GEOMETRY=AS_XYZ']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0RFbBpsIwNk"
   },
   "source": [
    "## **Data Manipulation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Load Boston Zoning file as dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the boston zoning layer features into a dataframe. As we can see there are 1649 parcels and 14 fields related to each one. \n",
    "You may check if the name of the csv file match the one that you have in your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_zoning = pd.read_csv(OUTPUT_FILES_PATH + \"boston_zoning.csv\")\n",
    "boston_zoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Load intersected layers files (Boston & Flood Hazard) as dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are loading the intersected layers attributes tables. You may check if the name of the csv file match the one that you have in your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "vLtaD8fJKM95",
    "outputId": "ca942b9f-1063-4331-a82d-12235ba3466d"
   },
   "outputs": [],
   "source": [
    "# Load Flood Hazard & Boston Zoning intersection file\n",
    "intersection_flz_boston = pd.read_csv(OUTPUT_FILES_PATH + \"intersection_flz_boston.csv\")\n",
    "# Load Sea Rise & Boston Zoning intersection file\n",
    "intersection_searise_boston = pd.read_csv(OUTPUT_FILES_PATH + \"intersection_searise_boston.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are printing each dataframe to make sure the data has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_flz_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_searise_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Join intersected layers files with the Boston Zoning layer file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flood Hazard Layer\n",
    "#To avoid duplication of columns we select only the columns that we want to preserve from the intersection\n",
    "columns_to_keep_flz = [\"fid\",\"flz_b_polyg_area_2\", \"FLD_ZONE\"]\n",
    "\n",
    "flz_boston_joined = boston_zoning.merge(\n",
    "    intersection_flz_boston[columns_to_keep_flz], \n",
    "    how=\"left\", \n",
    "    on=\"fid\", \n",
    "    \n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Sea Level Rise Layer\n",
    "#To avoid duplication of columns we select only the columns that we want to preserve from the intersection\n",
    "columns_to_keep_searise = [\"fid\",\"searise_polyg_area_2\"]\n",
    "\n",
    "searise_boston_joined = boston_zoning.merge(\n",
    "    intersection_searise_boston[columns_to_keep_searise], \n",
    "    how=\"left\", \n",
    "    on=\"fid\", \n",
    "    \n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flz_boston_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searise_boston_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Manage NaN values in searise_boston_joined dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the previous dataframe, the column searise_polyg_area has some NaN values. NaN values in this context are the parcels of the boston zoning layer that didn't intersect with sea rise level layer. So, we are going to replace this values with cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searise_boston_joined[\"searise_polyg_area_2\"] = searise_boston_joined[\"searise_polyg_area_2\"].apply(lambda x: 0 if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you print the searise_boston_joined dataframe you'll see that there are no NaN values anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searise_boston_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Delete Miscellaneous parcels from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flz_boston_joined = flz_boston_joined[flz_boston_joined[\"SUBDISTRIC\"]!=\"Miscellaneous\"]\n",
    "flz_boston_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Map flood zone hazard areas from letters to 100-year and 500-year string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flood hazard areas identified on the Flood Insurance Rate Map are identified as a Special Flood Hazard Area (SFHA). SFHAs are labeled as Zone A, Zone AO, Zone AH, Zones A1-A30, Zone AE, Zone A99, Zone AR, Zone AR/AE, Zone AR/AO, Zone AR/A1-A30, Zone AR/A, Zone V, Zone VE, and Zones V1-V30, etc. For more information go to: https://www.fema.gov/glossary/flood-zones \n",
    "\n",
    "We maped each label in one of these three categories: 500-year (label B), 100-year (labels B and C) and low-risk (all the rest of the labels). Now, it easier to score each parcel since we have only three labels instead of more than ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping parcel categories their respective type of flooding risk\n",
    "def map_flz_zone(zone_name):\n",
    "    if zone_name == \"B\":\n",
    "        return \"500-year\"\n",
    "    elif zone_name == \"C\" or zone_name == \"X\":\n",
    "        return \"low-risk\"\n",
    "    return \"100-year\" \n",
    "\n",
    "\n",
    "flz_boston_joined[\"FLD_ZONE_MAP\"] = flz_boston_joined[\"FLD_ZONE\"].apply(lambda x: map_flz_zone(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the label mapping in the following dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "id": "6KnTyU9oRD3k",
    "outputId": "9803a68f-33b0-42b2-cfc0-6ca7e5c4b4cd"
   },
   "outputs": [],
   "source": [
    "flz_boston_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Percentage overlap calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flood Zone Layer\n",
    "# Division between the flood zone area and the residential parcel area\n",
    "flz_boston_joined[\"flz_perc_overlap\"] = flz_boston_joined[\"flz_b_polyg_area_2\"]/flz_boston_joined[\"boston_polyg_area_2\"]\n",
    "\n",
    "\n",
    "# Sea Level Rise Layer\n",
    "# Division between the sea level rise area and the residential parcel area\n",
    "searise_boston_joined[\"searise_perc_overlap\"] = searise_boston_joined[\"searise_polyg_area_2\"]/searise_boston_joined[\"boston_polyg_area_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see now the percentage overlap calculation results as a new column in each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flz_boston_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searise_boston_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WW4_C0GbMTWQ"
   },
   "outputs": [],
   "source": [
    "#Flood zone score calculation\n",
    "flz_zone_type_scoring = {\n",
    "    \"500-year\": 0,\n",
    "    \"100-year\": 0 \n",
    "}\n",
    "def get_score_flz_zone(perc_overlap, flz_zone_type):\n",
    "    if perc_overlap <= 50:\n",
    "        return flz_zone_type_scoring[flz_zone_type]\n",
    "    return 0\n",
    "\n",
    "#Init with flz_zone_score 0 (when perc_overlap >50)\n",
    "flz_boston_joined[\"flz_zone_score\"] = 4\n",
    "\n",
    "condition_100_flz_zone = flz_boston_joined[\"FLD_ZONE_MAP\"] == \"100-year\" \n",
    "condition_500_flz_zone = flz_boston_joined[\"FLD_ZONE_MAP\"] == \"500-year\"\n",
    "\n",
    "flz_boston_joined.loc[condition_100_flz_zone, \"flz_zone_score\"] = flz_boston_joined[\"flz_perc_overlap\"].apply(\n",
    "    lambda x: get_score_flz_zone(x, \"100-year\")\n",
    ")\n",
    "flz_boston_joined.loc[condition_500_flz_zone, \"flz_zone_score\"] = flz_boston_joined[\"flz_perc_overlap\"].apply(\n",
    "    lambda x: get_score_flz_zone(x, \"500-year\")\n",
    ")\n",
    "\n",
    "\n",
    "# Areas of future development score calculation\n",
    "areas_of_future_dev_scoring = {\n",
    "    \"Industrial\" : 0,\n",
    "    \"Business\" : 0,\n",
    "    \"Comm/Instit\" : 0,\n",
    "    \"Mixed Use\" : 2,\n",
    "    \"Open Space\" : 4,\n",
    "    \"Residential\" : 4\n",
    "}\n",
    "def get_score_of_areas_of_future_dev(subdistric):\n",
    "    return areas_of_future_dev_scoring[subdistric]\n",
    "\n",
    "\n",
    "flz_boston_joined[\"future_dev_score\"] = flz_boston_joined[\"SUBDISTRIC\"].apply(\n",
    "    lambda x: get_score_of_areas_of_future_dev(x)\n",
    ")\n",
    "\n",
    "\n",
    "#Sea level rise score calculation\n",
    "def searise_score_calculation(perc_overlap):\n",
    "    if perc_overlap <= 50:\n",
    "        return 4\n",
    "    return 0\n",
    "\n",
    "searise_boston_joined[\"searise_score\"] = searise_boston_joined[\"searise_perc_overlap\"].apply(\n",
    "    lambda x: searise_score_calculation(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the code above, you now can see the score results for each layer as a new column in their respective dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flz_boston_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searise_boston_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to save individual score calculations dataframes to csv files. Pandas has an integrated function to do this, it is called **to_csv()**. You have to pass to this function the path where you want to save it and as an optional argument a boolean value to tell the function that you don't want to save the default id column to the file. \n",
    "\n",
    "You can modify the file name if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes with score calculations into csv files\n",
    "flz_boston_joined.to_csv(OUTPUT_FILES_PATH + \"intersection_flz_boston_scores.csv\", index=False)\n",
    "searise_boston_joined.to_csv(OUTPUT_FILES_PATH + \"intersection_searise_boston_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Join all intersected tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Flood Hazard layer intersection & Sea level rise intersection layers into a new dataframe\n",
    "columns_to_keep_searise_joined = [\"fid\",\"searise_score\"]\n",
    "all_individual_scores = flz_boston_joined.merge(\n",
    "    searise_boston_joined[columns_to_keep_searise_joined], \n",
    "    how=\"left\", \n",
    "    on=\"fid\", \n",
    ")\n",
    "all_individual_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saved the individual scores dataframes to csv file. We are doing the same with merged individual scores dataframe (all_individual_scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe with all individual score calculations into csv file\n",
    "all_individual_scores.to_csv(OUTPUT_FILES_PATH + \"all_individual_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Join intersected dataframes with boston zoning dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid duplication of columns we select only the columns that we want to preserve from the intersection\n",
    "columns_to_keep_flz = [\"fid\",\"flz_zone_score\", \"future_dev_score\", \"searise_score\"]\n",
    "\n",
    "total_scores = boston_zoning.merge(\n",
    "    all_individual_scores[columns_to_keep_flz], \n",
    "    how=\"left\", \n",
    "    on=\"fid\", \n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe below contains all the individual scores mergeg with the Boston zoning dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Fill NaN values of individual scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each individual scores, there are some NaN values. A NaN value represents that the parcel didn't overlap with the layer, so there is no risk. We'll fill these NaN values with a maximum score of each criterion plus one, which means that these parcels are the safest on each criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_flz_score = total_scores[\"flz_zone_score\"].max()\n",
    "total_scores[\"flz_zone_score\"] = total_scores[\"flz_zone_score\"].apply(lambda x: max_flz_score+1 if pd.isna(x) else x)\n",
    "\n",
    "max_future_dev_score = total_scores[\"future_dev_score\"].max()\n",
    "total_scores[\"future_dev_score\"] = total_scores[\"future_dev_score\"].apply(\n",
    "    lambda x: max_future_dev_score+1 if pd.isna(x) else x\n",
    ")\n",
    "\n",
    "max_searise_score = total_scores[\"searise_score\"].max()\n",
    "total_scores[\"searise_score\"] = total_scores[\"searise_score\"].apply(lambda x: max_searise_score+1 if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Total score calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are calculation the total score, it is obtained by summing up all individual scores. Then we present final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores[\"total\"] = total_scores[\"searise_score\"] + total_scores[\"flz_zone_score\"] + total_scores[\"future_dev_score\"]\n",
    "total_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Save final output to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores.to_csv(OUTPUT_FILES_PATH + \"total_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a special file to join the scores with the boston zoning layer\n",
    "total_scores_fields_to_join = total_scores[[\"fid\", \"searise_score\", \"flz_zone_score\", \"future_dev_score\", \"total\"]]\n",
    "total_scores_fields_to_join.to_csv(f\"{OUTPUT_FILES_PATH}total_scores_fields_to_join.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join scores csv file to boston zoning layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SCORES_CSV_PATH = f\"{OUTPUT_FILES_PATH}total_scores_fields_to_join.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading CSV files, the OGR driver assumes all fields are strings (i.e. text) unless it is told otherwise. \n",
    "You can create a CSVT file to tell OGR (and QGIS) what data type the different columns are.\n",
    "The CSVT file is a ONE line plain text file with the data types in quotes and separated by commas.\n",
    "This file is saved in the same folder as the .csv file, with the same name, but .csvt as the extension.\n",
    "Reference: https://docs.qgis.org/2.18/en/docs/user_manual/managing_data_source/supported_data.html#csvt-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvt_file = open(f\"{OUTPUT_FILES_PATH}total_scores_fields_to_join.csvt\", \"w\")\n",
    "csvt_file.write('\"Integer\",\"Integer\",\"Integer\",\"Integer\",\"Integer\"')\n",
    "csvt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_csv_layer = QgsVectorLayer(\n",
    "    TOTAL_SCORES_CSV_PATH,\n",
    "    \"scores_csv\",\n",
    "    \"ogr\")\n",
    "\n",
    "parameters_join = {\n",
    "    'INPUT': boston_zoning_layer,\n",
    "    'INPUT_2': scores_csv_layer,\n",
    "    'FIELD': 'fid',\n",
    "    'FIELD_2': 'fid',\n",
    "    'OUTPUT': f\"{OUTPUT_LAYERS_PATH}total_scores.gpkg\"\n",
    "}\n",
    "Processing.runAlgorithm(\"qgis:joinattributestable\", parameters_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these paths accordingly to your data location\n",
    "IMAGES_PATH = DATA_PATH + \"images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_layer_color_range(layer, ramp_name, value_field, inverse=False):\n",
    "    num_classes = 5\n",
    "    classification_method = QgsClassificationPrettyBreaks()\n",
    "\n",
    "    # change format settings as necessary\n",
    "    format = QgsRendererRangeLabelFormat()\n",
    "    format.setFormat(\"%1 - %2\")\n",
    "    format.setPrecision(2)\n",
    "    format.setTrimTrailingZeroes(True)\n",
    "\n",
    "    default_style = QgsStyle().defaultStyle()\n",
    "    color_map = None\n",
    "    if inverse:\n",
    "        color_ramp = default_style.colorRamp(ramp_name).invert()\n",
    "    else:\n",
    "        color_ramp = default_style.colorRamp(ramp_name)\n",
    "\n",
    "    renderer = QgsGraduatedSymbolRenderer()\n",
    "    renderer.setClassAttribute(value_field)\n",
    "    renderer.setClassificationMethod(classification_method)\n",
    "    renderer.setLabelFormat(format)\n",
    "    renderer.updateClasses(layer, num_classes)\n",
    "    renderer.updateColorRamp(color_ramp)\n",
    "\n",
    "    layer.setRenderer(renderer)\n",
    "    layer.triggerRepaint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_layer_to_image(layer, name, format=\"png\"):\n",
    "    # Script copied from: https://opensourceoptions.com/blog/pyqgis-render-print-save-a-layer-as-an-image/\n",
    "    # create image\n",
    "    img = QImage(QSize(1000, 1000), QImage.Format_ARGB32_Premultiplied)\n",
    "\n",
    "    # set background color\n",
    "    color = QColor(255, 255, 255, 255)\n",
    "    img.fill(color.rgba())\n",
    "\n",
    "    # create painter\n",
    "    p = QPainter()\n",
    "    p.begin(img)\n",
    "    p.setRenderHint(QPainter.Antialiasing)\n",
    "\n",
    "    # create map settings\n",
    "    ms = QgsMapSettings()\n",
    "    ms.setBackgroundColor(color)\n",
    "\n",
    "    # set layers to render\n",
    "    ms.setLayers([layer])\n",
    "\n",
    "    # set extent\n",
    "    rect = QgsRectangle(ms.fullExtent())\n",
    "    rect.scale(1.1)\n",
    "    ms.setExtent(rect)\n",
    "\n",
    "    # set ouptut size\n",
    "    ms.setOutputSize(img.size())\n",
    "\n",
    "    # setup qgis map renderer\n",
    "    render = QgsMapRendererCustomPainterJob(ms, p)\n",
    "    render.start()\n",
    "    render.waitForFinished()\n",
    "    p.end()\n",
    "\n",
    "    # save the image\n",
    "    img.save(f\"{IMAGES_PATH}{name}.{format}\")\n",
    "    print(f\"{IMAGES_PATH}{name}.{format} \\033[1m saved successfully \\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer(layer_path, provider=\"ogr\"):\n",
    "    layer = QgsVectorLayer(layer_path, provider)\n",
    "    if layer.featureCount() < 0:\n",
    "        raise ValueError(f\"The layer is empty, you may check layer path: {layer_path}\")\n",
    "    else:\n",
    "        print(f\"Layer {layer_path} \\033[1m loaded succesfully \\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key: name with which the image of the layer will be exported\n",
    "# value: layer\n",
    "layers_info_dict = {\n",
    "    \"sea_level_rise_layer\": sea_level_rise_layer,\n",
    "    \"boston_zoning_layer\": boston_zoning_layer,\n",
    "    \"flz_haz_layer\": flz_haz_layer,\n",
    "    \"intersection_flz_boston_layer\": intersection_flz_boston_layer,\n",
    "    \"intersection_searise_boston_layer\": intersection_searise_boston_layer,\n",
    "}\n",
    "total_scores = load_layer(f\"{OUTPUT_LAYERS_PATH}total_scores.gpkg\", \"ogr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name, layer in layers_info_dict.items():\n",
    "    export_layer_to_image(layer, image_name, format=\"png\")\n",
    "    \n",
    "set_layer_color_range(total_scores, \"YlOrRd\", \"total\", inverse=False)\n",
    "export_layer_to_image(total_scores, \"total_scores\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(f\"{IMAGES_PATH}total_scores.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits ##\n",
    "\n",
    "__Mentor__: Daniel Myers, GIS Analyst Boston Office of Emergency Management \n",
    "\n",
    "__Mentee__: Paulette Vazquez, Google Summer of Code 2021"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GIS_land_suitability_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
